{
  "schema_version": 1,
  "pdd_version": "0.0.133",
  "timestamp_utc": "20260201T025714Z",
  "argv": [
    "generate",
    "routerLogic3_python.prompt"
  ],
  "cwd": "/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts",
  "platform": {
    "system": "Darwin",
    "release": "24.5.0",
    "version": "Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:43 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8132",
    "python": "3.12.12 (main, Jan 27 2026, 23:41:44) [Clang 21.1.4 ]"
  },
  "global_options": {
    "force": false,
    "strength": null,
    "temperature": null,
    "time": 0.25,
    "verbose": false,
    "quiet": false,
    "local": false,
    "context": null,
    "output_cost": null,
    "review_examples": false
  },
  "invoked_subcommands": [
    "generate"
  ],
  "total_cost": 0.07572375,
  "steps": [
    {
      "step": 1,
      "command": "generate",
      "cost": 0.07572375,
      "model": "vertex_ai/gemini-3-pro-preview"
    }
  ],
  "errors": [],
  "environment": {
    "PATH": "/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts/mentorraEnv/bin:/Users/vincentsiu/.local/bin:/Users/vincentsiu/Library/Python/3.11/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/Users/vincentsiu/.nvm/versions/node/v22.21.0/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands",
    "VIRTUAL_ENV": "/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts/mentorraEnv",
    "PDD_PATH": "/Users/vincentsiu/.local/share/uv/tools/pdd-cli/lib/python3.12/site-packages/pdd",
    "PDD_ENV": "prod"
  },
  "file_contents": {
    ".pdd/meta/routerLogic_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-01-31T21:42:01.511247+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    ".pdd/meta/routerLogic2_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-01-31T23:41:12.881410+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    ".pdd/meta/routerLogic3_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-02-01T02:57:14.734143+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    "routerLogic3_python.prompt": "Can you modify the prompt for the toolhouse agent to include information of each of the mentors. \n\nAfter you have gathered info from the user, do an analysis against all mentors to see which would best suite the user.\nIn the required output format from the toolhouse agent, modify the output format to include a list of dictionaries that include suggested agents with the confidence percentage. The agents should be listed in descending order of confidence. \n\nDo not modify anything else other than the agent prompt.\n\nThe output from the ToolHouse agent should be:\n\"\n{\n  \"mentor_track\": \"string\",\n  \"switched_track\": boolean,\n  \"reply\": \"string\",\n  \"clarifying_question\": \"string or null\",\n  \"next_actions\": [\"action1\", \"action2\"],\n  \"suggested_agents\": [{\"agent1\":0.95},{\"agent2\":0.87}]\n  \"memory_update\": \"string summary of new facts\"\n}\n\"\n\nIntegrate the following information in the toolhouse prompt for it to do the analysis.\n1. **Vincent Forge \u2014 The Impossible Builder**\n   **Tagline:** Make the impossible inevitable through first principles and relentless execution\n   **Philosophy:** Most limits are just stacked assumptions. If physics allows it, it can be engineered\u2014aim for 10x and move fast.\n   **Best for:** Moonshots, first-principles problem solving, rapid iteration, scaling hard systems\n   **Style:** Direct, intense, impatient with excuses\n   **Signature question:** \u201cWhat\u2019s the actual constraint here?\u201d\n\n2. **Katerina Catalyst \u2014 The Scrappy Disruptor**\n   **Tagline:** Turn your struggles into your advantage\u2014bootstrap, hustle, believe\n   **Philosophy:** Great businesses come from personal pain points. Constraints create creativity\u2014start scrappy, sell early, and learn from \u201cno.\u201d\n   **Best for:** Bootstrapping, getting first customers, sales confidence, resilience through rejection\n   **Style:** Warm, encouraging, real\u2014pushes you without shaming you\n   **Signature question:** \u201cWhat would make YOU buy this?\u201d\n\n3. **Sophia Architect \u2014 The Experience Designer**\n   **Tagline:** Design experiences people love, not just products they use\n   **Philosophy:** Features are commodities; experiences create loyalty. Map the full journey and obsess over trust, delight, and story.\n   **Best for:** UX/customer journey, brand storytelling, trust & safety, differentiation through experience\n   **Style:** Thoughtful, human-centered, detail-obsessed\n   **Signature question:** \u201cWhat\u2019s the 11-star version of this?\u201d\n\n4. **Adrian Insight \u2014 The Startup Sage**\n   **Tagline:** Make something people want\u2014everything else is noise\n   **Philosophy:** Startups fail when they build in isolation. Talk to users, launch early, measure behavior, iterate until PMF.\n   **Best for:** Pre-PMF clarity, idea validation, pivot decisions, early execution strategy\n   **Style:** Calm, clear, direct\u2014cuts through hype\n   **Signature question:** \u201cDo people actually want this?\u201d\n\n\nCurrent Python Code below:\nimport os\nimport json\nimport io\nfrom typing import Optional, List\n\nfrom fastapi import FastAPI, HTTPException, UploadFile, File\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI, APIError\nfrom dotenv import load_dotenv\n\n# \u2705 Updated import (NO .client)\nfrom elevenlabs import ElevenLabs\n\n# Load environment variables\nload_dotenv()\n\n# Verify API Keys exist\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY is missing in .env\")\nif not os.getenv(\"ELEVENLABS_API_KEY\"):\n    raise ValueError(\"ELEVENLABS_API_KEY is missing in .env\")\n\napp = FastAPI(title=\"Mentorra Backend\")\n\n# Allow CORS for local frontend development\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialize Clients\nopenai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nelevenlabs_client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n\n# --- Data Models ---\n\nclass FounderProfile(BaseModel):\n    industry: Optional[str] = None\n    stage: Optional[str] = None\n    key_challenges: List[str] = Field(default_factory=list)\n\nclass UserRequest(BaseModel):\n    user_message: str\n    founder_profile: Optional[FounderProfile] = None\n    active_mentor_track: Optional[str] = None\n    memory_context: Optional[str] = \"\"\n\nclass MentorResponse(BaseModel):\n    mentor_track: str\n    switched_track: bool\n    reply: str\n    clarifying_question: Optional[str] = None\n    next_actions: List[str]\n    memory_update: str\n\nclass TTSRequest(BaseModel):\n    text: str\n    voice_id: Optional[str] = \"JBFqnCBsd6RMkjVDRZzb\"  # Default ElevenLabs voice\n    model_id: Optional[str] = \"eleven_monolingual_v1\"\n    output_format: Optional[str] = \"mp3_44100_128\"\n\n# --- Toolhouse Agent Logic ---\n\nSYSTEM_PROMPT_TEMPLATE = \"\"\"\nYou are the Mentorra Routing Agent. You act as the brain behind a founder's mentorship experience.\n\nYou will receive:\n- role: The current agent role (Router & Mentor Persona)\n- user_message: A single string from the founder\n- founder_profile: JSON summary of what we know so far\n- active_mentor: Current mentor track id if already selected\n- memory_context: Previous context of the conversation\n\nPrimary goals:\n1) Classify the user\u2019s message into exactly ONE mentor track (e.g., \"Product\", \"Sales\", \"Fundraising\", \"Leadership\", \"Growth\").\n2) Decide whether we should switch mentors or stay on the current one. Prefer stability unless the user\u2019s intent clearly changed.\n3) Reply as the selected mentor in a concise, supportive, operator style (no fluff).\n4) Ask at most ONE clarifying question, only if necessary to proceed.\n5) Provide 2\u20135 next actions that the founder can do immediately (this week).\n6) Update \"memory_update\" compactly so the next call stays consistent.\n\nOutput must be valid JSON matching this schema:\n{\n  \"mentor_track\": \"string\",\n  \"switched_track\": boolean,\n  \"reply\": \"string\",\n  \"clarifying_question\": \"string or null\",\n  \"next_actions\": [\"action1\", \"action2\"],\n  \"memory_update\": \"string summary of new facts\"\n}\n\"\"\"\n\n@app.post(\"/api/mentor-assist\", response_model=MentorResponse)\nasync def mentor_assist(request: UserRequest):\n    try:\n        user_input_context = f\"\"\"\n        INPUT DATA:\n        - User Message: \"{request.user_message}\"\n        - Active Mentor Track: {request.active_mentor_track if request.active_mentor_track else \"None\"}\n        - Founder Profile: {request.founder_profile.model_dump_json() if request.founder_profile else \"Unknown\"}\n        - Memory Context: {request.memory_context}\n        \"\"\"\n\n        try:\n            completion = openai_client.chat.completions.create(\n                model=\"gpt-4-turbo\",\n                response_format={\"type\": \"json_object\"},\n                messages=[\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE},\n                    {\"role\": \"user\", \"content\": user_input_context},\n                ],\n                temperature=0.7,\n            )\n        except APIError:\n            # Fallback to gpt-4o-mini\n            completion = openai_client.chat.completions.create(\n                model=\"gpt-4o-mini\", \n                response_format={\"type\": \"json_object\"},\n                messages=[\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE},\n                    {\"role\": \"user\", \"content\": user_input_context},\n                ],\n                temperature=0.7,\n            )\n\n        raw_content = completion.choices[0].message.content\n        data = json.loads(raw_content)\n        return MentorResponse(**data)\n\n    except Exception as e:\n        print(f\"Mentor Assist Error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# --- Voice Mode Endpoints ---\n\n@app.post(\"/api/voice/speak\")\nasync def text_to_speech(request: TTSRequest):\n    \"\"\"\n    Converts text into audio using ElevenLabs.\n    Streams MP3 audio back to the client.\n    \"\"\"\n    try:\n        audio_stream = elevenlabs_client.text_to_speech.convert(\n            voice_id=request.voice_id,\n            model_id=request.model_id,\n            text=request.text,\n            output_format=request.output_format,\n        )\n\n        def iterfile():\n            for chunk in audio_stream:\n                if chunk:\n                    yield chunk\n\n        return StreamingResponse(iterfile(), media_type=\"audio/mpeg\")\n\n    except Exception as e:\n        print(f\"TTS Error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Text-to-speech failed: {str(e)}\")\n\n@app.post(\"/api/voice/transcribe\")\nasync def speech_to_text(file: UploadFile = File(...)):\n    \"\"\"\n    Transcribes an uploaded audio file into text using OpenAI Whisper.\n    \"\"\"\n    try:\n        audio_bytes = await file.read()\n\n        audio_file = io.BytesIO(audio_bytes)\n        audio_file.name = file.filename or \"audio.mp3\"\n\n        transcript = openai_client.audio.transcriptions.create(\n            model=\"whisper-1\",\n            file=audio_file,\n        )\n\n        return {\"text\": transcript.text}\n\n    except Exception as e:\n        print(f\"STT Error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Transcription failed: {str(e)}\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    ".pdd/meta/routerLogic2Revision_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-02-01T01:54:09.225947+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}"
  },
  "terminal_output": "=== STDOUT ===\nChecking for updates...\n\nNew version of pdd-cli available: 0.0.135 (current: 0.0.133)\nWould you like to upgrade? [y/N]: \nUpgrade cancelled\nUsing .pddrc context: default\nInput files:\n  prompt_file     \n/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts/routerLogic3_python.prompt\nOutput files:\n  output          /Users/vincentsiu/Documents/personal_repo/mentorrahackaton/routerLogic3.py\nDetected language: python\nBasename: routerLogic3\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Starting prompt preprocessing                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Preprocessing complete                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Starting prompt preprocessing                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDoubling curly brackets...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Preprocessing complete                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nGenerated code saved to: /Users/vincentsiu/Documents/personal_repo/mentorrahackaton/routerLogic3.py\n\n--- Command Execution Summary ---\n  Step 1 (generate): Cost: $0.075724, Model: vertex_ai/gemini-3-pro-preview\nTotal Estimated Cost: $0.075724\n-------------------------------------\n"
}