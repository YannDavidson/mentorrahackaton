{
  "schema_version": 1,
  "pdd_version": "0.0.133",
  "timestamp_utc": "20260201T015409Z",
  "argv": [
    "generate",
    "routerLogic2Revision_python.prompt"
  ],
  "cwd": "/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts",
  "platform": {
    "system": "Darwin",
    "release": "24.5.0",
    "version": "Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:43 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8132",
    "python": "3.12.12 (main, Jan 27 2026, 23:41:44) [Clang 21.1.4 ]"
  },
  "global_options": {
    "force": false,
    "strength": null,
    "temperature": null,
    "time": 0.25,
    "verbose": false,
    "quiet": false,
    "local": false,
    "context": null,
    "output_cost": null,
    "review_examples": false
  },
  "invoked_subcommands": [
    "generate"
  ],
  "total_cost": 0.0032368499999999994,
  "steps": [
    {
      "step": 1,
      "command": "generate",
      "cost": 0.0032368499999999994,
      "model": "vertex_ai/gemini-3-pro-preview"
    }
  ],
  "errors": [],
  "environment": {
    "PATH": "/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts/mentorraEnv/bin:/Users/vincentsiu/.local/bin:/Users/vincentsiu/Library/Python/3.11/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/Users/vincentsiu/.nvm/versions/node/v22.21.0/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands",
    "VIRTUAL_ENV": "/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts/mentorraEnv",
    "PDD_PATH": "/Users/vincentsiu/.local/share/uv/tools/pdd-cli/lib/python3.12/site-packages/pdd",
    "PDD_ENV": "prod"
  },
  "file_contents": {
    ".pdd/meta/routerLogic2Revision_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-02-01T01:54:09.225947+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    ".pdd/meta/routerLogic_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-01-31T21:42:01.511247+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    "routerLogic2Revision_python.prompt": "The below router logic code needs 2 modifications.\n- The 'key_challenges' is a of type List[str], and should default to an empty list - field(default_factory=list) if necessary.\n- For the OpenAI, the model should fallback to GPT4.1-mini if the the current model  gpt4 turbo is not available for any reason.\n\nAsides from the two motifications needed, do not modify any other line\n\n\nbelow is the current code in Python\n\"\nimport os\nimport json\nimport io\nfrom typing import Optional, List\n\nfrom fastapi import FastAPI, HTTPException, UploadFile, File\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n# \u2705 Updated import (NO .client)\nfrom elevenlabs import ElevenLabs\n\n# Load environment variables\nload_dotenv()\n\n# Verify API Keys exist\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY is missing in .env\")\nif not os.getenv(\"ELEVENLABS_API_KEY\"):\n    raise ValueError(\"ELEVENLABS_API_KEY is missing in .env\")\n\napp = FastAPI(title=\"Mentorra Backend\")\n\n# Allow CORS for local frontend development\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialize Clients\nopenai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nelevenlabs_client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n\n# --- Data Models ---\n\nclass FounderProfile(BaseModel):\n    industry: Optional[str] = None\n    stage: Optional[str] = None\n    key_challenges: Optional[List[str]] = []\n\nclass UserRequest(BaseModel):\n    user_message: str\n    founder_profile: Optional[FounderProfile] = None\n    active_mentor_track: Optional[str] = None\n    memory_context: Optional[str] = \"\"\n\nclass MentorResponse(BaseModel):\n    mentor_track: str\n    switched_track: bool\n    reply: str\n    clarifying_question: Optional[str] = None\n    next_actions: List[str]\n    memory_update: str\n\nclass TTSRequest(BaseModel):\n    text: str\n    voice_id: Optional[str] = \"JBFqnCBsd6RMkjVDRZzb\"  # Default ElevenLabs voice\n    model_id: Optional[str] = \"eleven_monolingual_v1\"\n    output_format: Optional[str] = \"mp3_44100_128\"\n\n# --- Toolhouse Agent Logic ---\n\nSYSTEM_PROMPT_TEMPLATE = \"\"\"\nYou are the Mentorra Routing Agent. You act as the brain behind a founder's mentorship experience.\n\nYou will receive:\n- role: The current agent role (Router & Mentor Persona)\n- user_message: A single string from the founder\n- founder_profile: JSON summary of what we know so far\n- active_mentor: Current mentor track id if already selected\n- memory_context: Previous context of the conversation\n\nPrimary goals:\n1) Classify the user\u2019s message into exactly ONE mentor track (e.g., \"Product\", \"Sales\", \"Fundraising\", \"Leadership\", \"Growth\").\n2) Decide whether we should switch mentors or stay on the current one. Prefer stability unless the user\u2019s intent clearly changed.\n3) Reply as the selected mentor in a concise, supportive, operator style (no fluff).\n4) Ask at most ONE clarifying question, only if necessary to proceed.\n5) Provide 2\u20135 next actions that the founder can do immediately (this week).\n6) Update \"memory_update\" compactly so the next call stays consistent.\n\nOutput must be valid JSON matching this schema:\n{\n  \"mentor_track\": \"string\",\n  \"switched_track\": boolean,\n  \"reply\": \"string\",\n  \"clarifying_question\": \"string or null\",\n  \"next_actions\": [\"action1\", \"action2\"],\n  \"memory_update\": \"string summary of new facts\"\n}\n\"\"\"\n\n@app.post(\"/api/mentor-assist\", response_model=MentorResponse)\nasync def mentor_assist(request: UserRequest):\n    try:\n        user_input_context = f\"\"\"\n        INPUT DATA:\n        - User Message: \"{request.user_message}\"\n        - Active Mentor Track: {request.active_mentor_track if request.active_mentor_track else \"None\"}\n        - Founder Profile: {request.founder_profile.model_dump_json() if request.founder_profile else \"Unknown\"}\n        - Memory Context: {request.memory_context}\n        \"\"\"\n\n        completion = openai_client.chat.completions.create(\n            model=\"gpt-4-turbo\",\n            response_format={\"type\": \"json_object\"},\n            messages=[\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE},\n                {\"role\": \"user\", \"content\": user_input_context},\n            ],\n            temperature=0.7,\n        )\n\n        raw_content = completion.choices[0].message.content\n        data = json.loads(raw_content)\n        return MentorResponse(**data)\n\n    except Exception as e:\n        print(f\"Mentor Assist Error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# --- Voice Mode Endpoints ---\n\n@app.post(\"/api/voice/speak\")\nasync def text_to_speech(request: TTSRequest):\n    \"\"\"\n    Converts text into audio using ElevenLabs.\n    Streams MP3 audio back to the client.\n    \"\"\"\n    try:\n        audio_stream = elevenlabs_client.text_to_speech.convert(\n            voice_id=request.voice_id,\n            model_id=request.model_id,\n            text=request.text,\n            output_format=request.output_format,\n        )\n\n        def iterfile():\n            for chunk in audio_stream:\n                if chunk:\n                    yield chunk\n\n        return StreamingResponse(iterfile(), media_type=\"audio/mpeg\")\n\n    except Exception as e:\n        print(f\"TTS Error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Text-to-speech failed: {str(e)}\")\n\n@app.post(\"/api/voice/transcribe\")\nasync def speech_to_text(file: UploadFile = File(...)):\n    \"\"\"\n    Transcribes an uploaded audio file into text using OpenAI Whisper.\n    \"\"\"\n    try:\n        audio_bytes = await file.read()\n\n        audio_file = io.BytesIO(audio_bytes)\n        audio_file.name = file.filename or \"audio.mp3\"\n\n        transcript = openai_client.audio.transcriptions.create(\n            model=\"whisper-1\",\n            file=audio_file,\n        )\n\n        return {\"text\": transcript.text}\n\n    except Exception as e:\n        print(f\"STT Error: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Transcription failed: {str(e)}\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n\n\"",
    ".pdd/meta/routerLogic2_python.json": "{\n  \"pdd_version\": \"0.0.133\",\n  \"timestamp\": \"2026-01-31T23:41:12.881410+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}"
  },
  "terminal_output": "=== STDOUT ===\nChecking for updates...\n\nNew version of pdd-cli available: 0.0.135 (current: 0.0.133)\nWould you like to upgrade? [y/N]: \nUpgrade cancelled\nUsing .pddrc context: default\nInput files:\n  prompt_file     \n/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/prompts/routerLogic2Revision_python.promp\nt\nOutput files:\n  output          /Users/vincentsiu/Documents/personal_repo/mentorrahackaton/routerLogic2Revision.py\nDetected language: python\nBasename: routerLogic2Revision\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Starting prompt preprocessing                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Preprocessing complete                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Starting prompt preprocessing                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDoubling curly brackets...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Preprocessing complete                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nGenerated code saved to: \n/Users/vincentsiu/Documents/personal_repo/mentorrahackaton/routerLogic2Revision.py\n\n--- Command Execution Summary ---\n  Step 1 (generate): Cost: $0.003237, Model: vertex_ai/gemini-3-pro-preview\nTotal Estimated Cost: $0.003237\n-------------------------------------\n"
}